\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{latexsym, amssymb, amscd, amsthm, amsxtra, amsmath,amsthm }
\usepackage{graphics, graphicx, color}
\usepackage{natbib}
\usepackage{ifpdf}
\usepackage[format=hang,indention=-1cm,small]{caption}
\usepackage[caption=false]{subfig}
\usepackage{multirow}
\usepackage{kotex}
\usepackage{geometry}
\geometry{
	a4paper,
	left=30mm,
	right=30mm,
	top=30mm,
	bottom=30mm
}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=blue,
    citecolor=blue,
    pdfpagemode=FullScreen,
    }
\usepackage{indentfirst}


\title{2022 IDL Project Proposal \\ \vspace{0.5cm} \large \textbf{Microsoft Teams to Cloud Speech Enhancement}}
\author{Brian Lim, Chanwoo Kim, Taeyoung Chang, Urvish Takker \\ Carnegie Mellon University}
\date{\today}
\date{}
% MATH -----------------------------------------------------------
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Real}{\mathbb R}
\newcommand{\Nat}{\mathbb N}
\newcommand{\Int}{\mathbb Z}
\newcommand{\Complex}{\mathbb C}
\newcommand{\eps}{\varepsilon}
\newcommand{\To}{\longrightarrow}

\def\av{\mathbf a}
\def\bv{\mathbf b}
\def\cv{\mathbf c}
\def\dv{\mathbf d}
\def\ev{\mathbf e}
\def\fv{\mathbf f}
\def\gv{\mathbf g}
\def\hv{\mathbf h}
\def\iv{\mathbf i}
\def\jv{\mathbf j}
\def\gv{\mathbf g}
\def\kv{\mathbf k}
\def\lv{\mathbf l}
\def\mv{\mathbf m}
\def\nv{\mathbf n}
\def\ov{\mathbf o}
\def\pv{\mathbf p}
\def\qv{\mathbf q}
\def\rv{\mathbf r}
\def\sv{\mathbf s}
\def\tv{\mathbf t}
\def\uv{\mathbf u}
\def\vv{\mathbf v}
\def\wv{\mathbf w}
\def\xv{\mathbf x}
\def\yv{\mathbf y}
\def\zv{\mathbf z}

\def\Av{\mathbf A}
\def\Bv{\mathbf B}
\def\Cv{\mathbf C}
\def\Dv{\mathbf D}
\def\Ev{\mathbf E}
\def\Fv{\mathbf F}
\def\Gv{\mathbf G}
\def\Hv{\mathbf H}
\def\Iv{\mathbf I}
\def\Jv{\mathbf J}
\def\Kv{\mathbf K}
\def\Lv{\mathbf L}
\def\Mv{\mathbf M}
\def\Nv{\mathbf N}
\def\Ov{\mathbf O}
\def\Pv{\mathbf P}
\def\Qv{\mathbf Q}
\def\Rv{\mathbf R}
\def\Sv{\mathbf S}
\def\Tv{\mathbf T}
\def\Uv{\mathbf U}
\def\Vv{\mathbf V}
\def\Wv{\mathbf W}
\def\Xv{\mathbf X}
\def\Yv{\mathbf Y}
\def\Zv{\mathbf Z}

\newcommand{\alphav}{\mbox{\boldmath{$\alpha$}}}
\newcommand{\betav}{\mbox{\boldmath{$\beta$}}}
\newcommand{\gammav}{\mbox{\boldmath{$\gamma$}}}
\newcommand{\deltav}{\mbox{\boldmath{$\delta$}}}
\newcommand{\epsilonv}{\mbox{\boldmath{$\epsilon$}}}
\newcommand{\zetav}{\mbox{\boldmath$\zeta$}}
\newcommand{\etav}{\mbox{\boldmath{$\eta$}}}
\newcommand{\iotav}{\mbox{\boldmath{$\iota$}}}
\newcommand{\kappav}{\mbox{\boldmath{$\kappa$}}}
\newcommand{\lambdav}{\mbox{\boldmath{$\lambda$}}}
\newcommand{\muv}{\mbox{\boldmath{$\mu$}}}
\newcommand{\nuv}{\mbox{\boldmath{$\nu$}}}
\newcommand{\xiv}{\mbox{\boldmath{$\xi$}}}
\newcommand{\omicronv}{\mbox{\boldmath{$\omicron$}}}
\newcommand{\piv}{\mbox{\boldmath{$\pi$}}}
\newcommand{\rhov}{\mbox{\boldmath{$\rho$}}}
\newcommand{\sigmav}{\mbox{\boldmath{$\sigma$}}}
\newcommand{\tauv}{\mbox{\boldmath{$\tau$}}}
\newcommand{\upsilonv}{\mbox{\boldmath{$\upsilon$}}}
\newcommand{\phiv}{\mbox{\boldmath{$\phi$}}}
\newcommand{\varphiv}{\mbox{\boldmath{$\varphi$}}}
\newcommand{\chiv}{\mbox{\boldmath{$\chi$}}}
\newcommand{\psiv}{\mbox{\boldmath{$\psi$}}}
\newcommand{\omegav}{\mbox{\boldmath{$\omega$}}}
\newcommand{\Sigmav}{\mbox{\boldmath{$\Sigma$}}}
\newcommand{\Lambdav}{\mbox{\boldmath{$\Lambda$}}}
\newcommand{\Deltav}{\mbox{\boldmath{$\Delta$}}}

\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}

\begin{document}

\pagenumbering{gobble}

\maketitle

\begin{abstract}
    With an increasing trend in remote work, real-time speech enhancement models are becoming important in conferencing platforms for improving the experience of online communication. However, the difficulty comes in the real-time aspect of these models, as it is hard to expect noise that arises in the future. Because noises can provide unsatisfactory quality of speech and eventually disrupt conversations, suppressing them is crucial for a stable meeting. Our proposed model, <>, intended to promote collaborative research in real-time Speech Enhancement aimed to maximize the subjective (perceptual) quality of the enhanced speech (add how the model would process audio to extract clean speech). Our model aims to take a spectrogram of an audio that consists of both clean speech and artificial noise and To analyze the efficacy of our proposed model, we use metrics that tend to be better predictors of real-time performance over conventional objective metrics. For evaluation, we use a dataset generated using a large clean speech and noise corpus from the DNS challenge(\cite{DNSchallenge2020}) for training the proposed noise suppression model and representative of real-world noisy data. (The paper would contain descriptions of the dataset, proposed model, and results of the test on the proposed model and metrics.)
\end{abstract}


\section{Introduction}
Speech enhancement has become one of the most important technologies in a remote working environment.  Meetings done through video conferencing platforms such as Zoom or Microsoft Teams are more prevalent than ever, and extracting clean speech from a noisy environment in real time is becoming more important to avoid interruptions during a conference. However, solely extracting clean speech in real-time is a difficult task, especially with imperfect information on the level of noise in the future. Furthermore, suppressing noise can also lead to the suppression of clean speech as well, removing essential parts of a conversation. Losing essential signals from speech may further degrade its quality, and to prevent this, noise must be filtered out the moment it is detected in audio. Thus, we want to build a model which would perform better in real time. 

Our goal is that when we get noisy speech data derived from synthesizing clean speech and noise, we want to build a model (where the basic structure is FullSubnet and some properties of Demucs are added in a clever way? Not determined yet…) that yields a speech close to the clean speech. In other words, we want to build a model that fulfills a noise suppression while perceptual quality and intelligibility of the output is at least competitive with the noise suppression output of Microsoft Teams. 

Here, the input is a spectrogram transformed from the synthesized speech file. The spectrogram can be viewed as a 2D tensor of a shape with (T, F). For each time $t \in \{1, \cdots, T\}$, the wave form of short time segment near $t$ is Fourier-transformed (STFT) and the absolute value of this short term fourier transform at each frequency $f$ is stored as F-dimensional vector. Our model is defined as mapping a spectrogram of noisy speech data to the training target cIRM which stands for complex Ideal Ratio Mask(\cite{CIRM}). Since the desired output of cIRM can be viewed as the complex ratio of clean speech spectrum to noisy speech spectrum, training via the loss of measuring distance of actual output and the desired output can be interpreted as knowing well where the noise exists in time-frequency domain so that we can suppress such noise.




\section{Related Works}

\subsection{Demucs}

The authors of this paper(\cite{Demucs}) utilize convolutional and recurrent neural networks to separate noise from the clean speech in waveform audio. By processing corrupted audio through convolutional encoder-decoder architecture with skip-connections they propose that clean speech can be extracted from latent representations of input audio. Furthermore, to account for sequential information in the audio a recurrent neural network is injected between encoders and decoders, which the decoder uses to reconstruct clean speech. To account for real-time noise suppression the authors accumulate standard deviations of audios only up to the current position for normalization, with paddings to account for a 3ms lookahead.


\subsection{FullSubNet}

This paper(\cite{fullsubnet}) introduces a full-band and sub-band fusion model, FullSubNet, which enhances single-channel speech. Full-band model has its strength in finding global spectral patterns and cross-band dependencies. On the other hand, sub-band model has its strength in finding local spectral patterns and stationarity, which is an important factor that distinguishes noises from clean speech. Since both models have their own advantages, FullSubNet stacks both models to take both advantages. The whole speech spectrogram goes through a full-band model, and the output is concatenated with sub-band unit. Then the concatenated ones become frequency-wise inputs of the sub-band model. Hence, the sub-band model can find local patterns with complementary information captured from the full-band model. Besides, this model exceeds the high-ranked models in the DNS Challenge (INTERSPEECH 2020).

\subsection{GeMAPS}

This paper(\cite{GeMAPS}) suggests a minimalistic set of standard acoustic parameters such as Pitch, Jitter, Shimer, Loudness, Harmonics-to-Noise Ratio, and Harmonic difference. It is selected based on three criteria: the potential to discern physiological changes in voice production, the success in the past literature, and the theoretical importance. Through the experiments, the paper argues that, unlike the large brute-forced feature sets, the minimalistic parameter sets might reduce the danger of damaging generalization capabilities to unseen data. The authors proposed these parameters as the common baseline for evaluating future research. Using those metrics, we can measure the subjective qualities of the output from our model.

\section{Dataset}

We will use the dataset from the github repository of \href{https://github.com/microsoft/DNS-Challenge}{DNS Challenge 2020}. There is an explanation for the dataset in \cite{DNSchallenge2020}. 

For training data, we will get the noisy speech dataset by synthesizing clean speech and noise. The clean speech dataset comes from the public audiobooks dataset called Librivox. It has recordings over 10,000 public domain audiobooks by 11,350 speakers where majority of them are in English. Many of these recordings have excellent speech quality, but still some are of poor speech quality with speech distortion, background noise and reverberation. Thus, DNS Challenge 2020 has filtered the clean speech data based on the speech quality, which is measured by the Mean Opinnion Score (MOS), and chose only the upper quartile with respect to MOS as the clean speech dataset. The resulting dataset has 500 hours of speech from 2150 speakers and all the filtered clips are split into segments of 10 seconds.

The noise clips were selected from Audioset and Freesound. Audioset is a collection of about 2 million human-labeld 10 second sound clips. They are drawn from YouTube videos and belong to about 600 audio events. Since certain audio event classes such as music and speech are overrepresented in the Audioset, DNS Challenge 2020 has tried to balance the dataset by sampling so that each class audio event class has at least 500 clips. Also, DNS Challenge 2020 has removed the clips with any kind of speech activity since speech-like noise can bring out the suppression of speech while the trained model tries to suppress speech-like noise. The resulting noise dataset has about 150 audio classes and 60,000 clips. An additional 10,000 noise clips from Freesound are also augmented to the dataset.

The noisy speech dataset is created by adding clean speech and noise at various SNR levels. (Synthesizing the clean speech and noise : same method as the paper? Or new method?)

Although what we observe in the real world is not perfectly expressed by the synthetic dataset, we take advantage of the synthetic dataset since most of the speech enhancement (SE) models require a clean reference for utilizing objective metrics such as PESQ or STOI.

For the test set, we can use synthetic test dataset which adds the clean speech from the Graz University's clean speech dataset and noise clips from the Audioset and Freesound, which are not present in the training set. Since these synthetic clips come with ground truth references which are the clean speech data, we can evaluate the method using objective metrics such as PESQ and STOI. Whereas for the test set, or `blind' test set, there is no ground truch references provided. We can use this set for the final evaluation using subjective metrics.

Here, for our problem, we can write $x = y+n$ where $y$ is the clean speech, $n$ is the noise, and the $x$ is the synthesized noisy speech. Simply put, $x$ is the input for our model and the $y$ is the desired output of our model. If we denote $\hat y$ as the actual output of our model and $y^\star$ as the output from the Microsoft Teams platform, then we want for the divergence of $\hat y$ and $y$  to be smaller than that of $y^ \star$ and $y$. In this way, we can measure how well our noise suppression model works compared to Microsoft Teams.

\section{Evaluation Metric}

\begin{tabular}{ |p{2.5cm}||p{11cm}|}
    \hline
    \multicolumn{2}{|c|}{Evaluation Metric List} \\
    \hline
    Metric &  Mathematical Expression\\
    \hline
    \vspace{2.5cm}\textbf{STOI}  & $$STOI = \frac{1}{JM}\sum_{j, m} d_{j,m}$$  \begin{itemize}
        \item $J$ : the number of one-third octave bands (frequency)
        \item $M$ : the total number of frames (time)
        \item $d_{j,m}$ : sample correlation between $\xv_{j,m}$ and $\overline{\yv}_{j,m}$
        \item $\xv_{j,m}$ : the short-time temporal envelope of the clean speech
        \item $\overline{\yv}_{j,m}$ : the short-time temporal envelop of the (normalized and clipped) degraded speech 
    \end{itemize} \\ 
    \hline
    \vspace{1.5cm}\textbf{PESQ} &   $$PESQ = a_0 + a_1 D_{ind} + a_2 A_{ind}$$
    \begin{itemize}
        \item $D_{ind}$ : average disturbance
        \item $A_{ind}$ : asymmetric average disturbance 
        \item $a_0, a_1, a_2$ : $a_0$ is set as 4.5 and $a_1, a_2 <0$ can be modified for the given task
    \end{itemize}  \\
    \hline 
    \vspace{2.5cm} \textbf{fwSegSNR} & $$fwSegSNR = \frac{10}{M} \sum_{m=1}^{M-1}\frac{\sum_{j=1}^K W(j,m)\log_{10} \frac{\abs{X(j,m)}^2}{(\abs{X(j,m)} - \abs{\hat X(j,m)})^2} }{\sum_{j=1}^K W(j,m)} $$
    \begin{itemize}
        \item $K$ : the number of bands 
        \item $M$ : total number of frames 
        \item $\abs{X}$ : weighted (by a Gaussian shaped window) clean signal spectrum 
        \item $W$ : weight on $j$-th frequency and $m$-th frame, which is exponential $\abs{X}$
    \end{itemize}\\
    \hline
\end{tabular}

\vspace{1cm}

STOI(\cite{STOI}) stands for a short-time objective intelligence measure. Essentially it measures the average of every correlation coefficient between each of short-time segments of DFT(discrete Fourier transform)-based band for clean speech and the output of noise reduction from the noisy speech input. 

Perceptual Evaluation of Speech Quality (PESQ)(\cite{PESQ}) is a metric used for obtaining Mean Opinion Scores (MOS) of speech in an audio. This objective metric, ranging from -0.5 to 4.5, measures the quality of an audio based on a sum of disturbances in the audio, and while it is not intended for speech enhanced by noise suppression, we follow \cite{microsoftTeams}, our baseline model, to compare our model’s performance.

SDR or Speech to Distortion Ratio: Distortion is unwanted signal correlated with the input signal. It's absent until the signal appears, and is usually not significant until the signal is occupying most of the dynamic range of the system. This can be a very good metric to track as distortion is something we can reduce and it might lead to clean speech results.

fwSegSNR(\cite{evalMetrics}), Frequency weighted Segmental Signal-to-Noise Ratio, is a generalized short time performance measure. It gives each frequency’s SNR two kinds of weights. First, static frequency weighting that is derived from known psycho-acoustic properties of hearing. Second, dynamic frequency weighting which is related to the speech production mechanism.

\section{Baseline}

Microsoft Teams is a business communication platform developed by Microsoft, of which usage share keeps increasing so that its number of users reaches 270million in 2022 (\href{(https://www.businessofapps.com/data/microsoft-teams-statistics/)}{Source}). It mainly provides lots of features that support live speech in meetings, calls, and so on. Hence, the need for speech enhancement is inevitable. So we aim to improve it more and adopt Microsoft Teams as the baseline of our model. Although papers from Microsoft including \cite{microsoftTeams} suggest Microsoft’s research into an extension of the Demucs architecture, the exact model used in Microsoft Teams is not publicly available due to the confidentiality of their research. However, we can still set our model’s baseline as the model used in Microsoft Teams by obtaining the suppressed audio by inputting the audio through their platform. To minimize the noise that could come from recording audio through a microphone, we inject the corrupted audio through a virtual microphone to simulate the inference stage of inputting audio files into Microsoft’s model.



\bibliographystyle{unsrtnat}
\bibliography{references}




\end{document}
